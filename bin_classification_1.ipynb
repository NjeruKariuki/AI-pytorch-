{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Binary classification -> Neural networks.\n",
    "###classifying circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##dependencies\n",
    "from sklearn.datasets import make_circles #generate circles\n",
    "import pandas as pd ##creating data frame\n",
    "import matplotlib.pyplot as plt ##for drawing plots\n",
    "import torch #creating tensors\n",
    "from torch import nn ## basic building block of neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make   1000 samples \n",
    "n_samples = 1000\n",
    "x, y = make_circles(\n",
    "    n_samples,\n",
    "    noise=0.02,\n",
    "    random_state=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualizing my data\n",
    "##dataframe\n",
    "circles = pd.DataFrame({\n",
    "    'X1': x[:, 0], \n",
    "    'X2': x[:, 1],\n",
    "    'Labels': y\n",
    "})\n",
    "\n",
    "circles.head(10)\n",
    "##circles['Labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### drawing plots\n",
    "plt.scatter(\n",
    "    x=x[:,0],\n",
    "    y=x[:,1],\n",
    "    s=8,\n",
    "    c=y,\n",
    "    cmap=plt.cm.RdYlBu\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##turn data into training and testing data\n",
    "###input and output shapes \n",
    "##x.shape, y.shape\n",
    "\n",
    "x_tensor = torch.from_numpy(x).type(torch.float)\n",
    "y_tensor = torch.from_numpy(y).type(torch.float)\n",
    "\n",
    "#x_tensor.shape, y_tensor.shape\n",
    "#x_tensor, y_tensor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    x_tensor,\n",
    "    y_tensor, \n",
    "    test_size=0.2, #20% test data, 80% training data\n",
    "    random_state=30\n",
    ")\n",
    "\n",
    "print(f\"X training data: {len(X_train)}\\nX Test Data: {len(X_test)}\\n Y_train Data: {len(Y_train)}\\n Y Test DATA: {len(Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##device agnostic\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create a model\n",
    "model_0 = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=5), #2 inputs (x) with 1 hidden layer (5 neurons)\n",
    "    nn.Linear(in_features=5, out_features=1) #1 output y\n",
    ").to(device)\n",
    "\n",
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the model\n",
    "untrained_preds = model_0(X_test.to(device))\n",
    "print(f\"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}\")\n",
    "print(f\"Length of test samples: {len(Y_test)}, Shape: {Y_test.shape}\")\n",
    "print(f\"\\nFirst 10 predictions:\\n{untrained_preds[:10]}\")\n",
    "print(f\"\\nFirst 10 test labels:\\n{Y_test[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##loss function and optimizer\n",
    "#loss_fn = torch.nn.BCELoss() # no sigmoid built in\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss() #sigmoid built in\n",
    "\n",
    "#optimizer\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)\n",
    "\n",
    "#function to calculate accuracy of model\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() #find number of correct predictions\n",
    "    acc = (correct/len(y_pred))*100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##logits are unnormalized final scores of your model -> source: data science stack exchange\n",
    "##training and testing our model\n",
    "\n",
    "torch.manual_seed(40)\n",
    "\n",
    "epochs = 300 #train with 100 epochs \n",
    "\n",
    "#put train data and test data to device\n",
    "X_train, Y_train = X_train.to(device), Y_train.to(device)\n",
    "X_test, Y_test = X_test.to(device), Y_test.to(device)\n",
    "\n",
    "#training loop\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_0.train() #training mode\n",
    "\n",
    "    #forward pass\n",
    "    y_logits = model_0(X_train).squeeze() #squeeze to remove extra dimensions\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) #normalize using sigmoid fxn -> round to get prediction probabalities using round\n",
    "\n",
    "    #calculate the loss\n",
    "    loss = loss_fn(y_logits, Y_train) #not using sigmoid since BCEWithLogitsLoss has inbuilt sigmoid \n",
    "\n",
    "    acc = accuracy_fn(y_true=Y_train, \n",
    "    y_pred=y_pred)\n",
    "\n",
    "    #optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #loss backwards \n",
    "    loss.backward()\n",
    "\n",
    "    #optmizer step -> apply gradient descent\n",
    "    optimizer.step()\n",
    "\n",
    "    ###testing \n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        #inference\n",
    "        test_logits = model_0(X_test).squeeze()\n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "        \n",
    "        #calculate test loss\n",
    "        test_loss = loss_fn(test_logits, Y_test) \n",
    "\n",
    "        #test accuracy\n",
    "        test_acc = accuracy_fn(\n",
    "            y_true=Y_test,\n",
    "            y_pred=test_pred\n",
    "        )\n",
    "\n",
    "    #print out whats happening \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} ~ Loss: {loss:.5f}, Accuracy: {acc:.2f} ~ Test Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing predictions\n",
    "import requests\n",
    "from pathlib import Path \n",
    "'''\n",
    "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    f.write(request.content)\n",
    "'''\n",
    "\n",
    "from helper_functions import plot_predictions, plot_decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61b64fdf340dd4dcb83d9f0d8994a49c4a1442336d2c73dd1f85f0966d40bb2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
